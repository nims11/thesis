% Parent problem, approaches, CAL and its history
High recall information retrieval is crucial to tasks such as electronic
discovery and systematic review - where the goal is to find all or nearly all
relevant documents using minimal human effort. Technical Assisted Review (TAR)
methods outperform manual review in legal eDiscovery by reducing the cost spent
on human reviewers~\cite{grossman2010technology,roitblat2010document}.  In a TAR
process, a computer system uses judgments made by human reviewers to classify
documents as either relevant or non-relevant. Continuous active learning
(CAL)~\cite{cormack2014evaluation} is a TAR protocol where a machine learning
algorithm suggests most likely relevant documents for review and continuously
incorporates relevance feedback to improve its understanding of the search task.
In a previous study, \citet{cormack2014evaluation} showed that CAL outperforms other TAR protocols on
review tasks from actual legal matters and TREC 2009 Legal
Track.  The Total Recall track in TREC 2015 and
2016 evaluated different systems under a simulated TAR
setting~\cite{grossman2016trec,roegiest2015trec}.  Baseline Model Implementation
(BMI) based on CAL was used as the baseline in these tracks. BMI implements the
AutoTAR algorithm.  None of the participating systems were able to consistently
outperform the baseline. In this paper, we modify and extend the AutoTAR
algorithm.

% The issue of scaling for large datasets crop up in multiple parts of this paper.
% Scalable Continuous Active Learning (or ``S-CAL'')~\cite{cormack2016scalability}
% is designed to work with large document collections where it is desirable to
% build an effective classifier using minimal labelling effort.

% What is refreshing
\textit{Refreshing} is an important step in the CAL process. During a
\textit{refresh}, the relevance judgments from the reviewer are used to train a
new classifier. This classifier generates an ordered list of documents most
likely to be relevant, which is later processed by the reviewer. This step has a
high computation cost because it involves training a classifier and computing
relevance likelihood scores for potentially all the documents in the corpus. A
\textit{refresh strategy} determines when and how to perform the refresh. By
studying various refresh strategies, we aim to:

\begin{itemize} \item Improve effectiveness of CAL; specifically its capability
to achieve higher recall with lesser effort \item Improve computational
efficiency of CAL; so that it is responsive and  feasible in a production
environment.  \end{itemize}

In this paper, we propose different refresh strategies and compare their effect on
the behaviour of CAL. By default, refreshing in AutoTAR is done after a
certain number of judgments is
received. This number increases exponentially over time. We find
we can make CAL more effective by refreshing after every judgment.
However, this comes with significant computation overhead, which with careful
implementation and modern hardware, can be minimized for reasonably sized
datasets. We also discuss other refresh strategies which have lower computation
cost and achieve similar effectiveness. These strategies can be considered when
dealing with resource constraints or large datasets.
