\label{chap:implementation}
In this chapter, we discuss our implementation of CAL. We used this
implementation to perform all the experiments described in the future chapters.
We start by reviewing an early implementation of CAL, followed by our goals
and expectations from the new implementation. We then briefly discuss the
design decisions, along with the capabilities of our system.

The code for our system is open source and available with documentation
online\footnote{\url{https://hical.github.io/}}.

\section{Auto-TAR BMI}
As mentioned in the previous chapter,
BMI(Baseline Model
Implementation) is an implementation of CAL which was used as a baseline in the
TREC 2015 and 2016 Total Recall track. Most of BMI is written as
a BASH script, which coordinates various external C/C++ programs to perform more
more specific tasks like training and scoring. The intermediate data is
stored in text files since BASH is not designed to support advanced data
structures.

The AutoTAR BMI was released by the Total Recall track organizers as a virtual
machine\footnote{\url{https://plg.uwaterloo.ca/~gvcormac/trecvm/}}. A local
version of the tool also
available\footnote{\url{https://github.com/HTAustin/CAL}}.

\section{A modern CAL implementation}

\subsection{Motivation and Goals}
The design of BMI limit us from designing refresh strategies which require more
computation or involve complex logic. Moreover, while BMI is suitable for
simulation purposes, it is difficult to use in real world applications.

As part of our TREC 2017 Core effort~\cite{zhang2017uwaterloomds}, we designed a
CAL-powered review tool called HiCAL~\cite{sigirdemo}. We intended this tool to
process relevance feedback as quickly as possible (in other words, more frequent
refreshes). For a smooth user experience, we wanted to achieve this with minimum
possible system delay. Due to these reasons, we decided to implement CAL from
scratch in C++. We chose C++ because it provides a good control over efficiency
and is usually easy to maintain/extend.

We wanted to build a system which could satisfy all the requirements of
HiCAL, and could be easily used for other applications and future research, such as
the work presented in this thesis. We designed this system to meet the following
goals:

\begin{itemize}
\item Fast and efficient.
\item Support parallel tasks (for parallel simulations and multiple users).
\item Easy to use as a standalone tool or as a part of an external application.
\item Easy to extend and modify any step of the CAL algorithm.
\end{itemize}

\subsection{Design}
\label{sec:cal.design}
In this section, we briefly discuss the design details of our CAL system.

Prior to using the CAL system, a one-time preprocessing step is required to convert a
given text collection to a machine readable set of feature vectors. The corpus
processor takes as input an archive (a tar.gz file) of text documents, computes
the feature vectors for every document, and writes them to a binary file. We use
binary files over human-readable svmlight files~\cite{joachims1999svmlight} to
significantly improve the output file size and loading times. When working with
the athome1 test collection (see Chapter~\ref{chap:dataset} for more details),
the document feature vectors when stored in the svmlight format took 734
megabytes of disk space and 8.6 seconds to process+load into the memory by
the CAL system. On the same machine, our binary file format took 341 megabytes
of disk space and 2.7 seconds to process+load into the memory. The athome1
collection has only 290k documents and these differences become more significant
when working with larger collections. For the New York Times collection which
has around 1.8 million documents, the svmlight format took 7.6 gigabytes of disk
space and 90.1 seconds to process+load into the memory, while our binary file
format took 3.5 gigabytes of disk space and 30.1 seconds to process+load into the
memory.

Each document is assigned an ID which is its base
filename in the input archive (filename ignoring the directory structure). This
ID is used to judge and retrieve documents in the CAL system. The files are
treated as plaintext files and are assumed to be cleaned as per the needs of the
user.  Figure~\ref{fig:preprocessing} shows a preprocessing pipeline example
with the New York Times collection.  By default, the corpus preprocessor computes
tf-idf unigram features for every document as explained in Section~\ref{sec:cal}.

\begin{figure}[h]
\includegraphics[width=\textwidth]{figures/corpus_pipeline.pdf}
\caption{Preprocessing pipeline for the New York Times collection}
\label{fig:preprocessing}
\end{figure}

The output binary file contains document frequency data followed by the document
feature data. The binary file format is described in
Figure~\ref{fig:binary_format}.

\begin{figure}[h]
\includegraphics[width=\textwidth]{figures/binary_format.pdf}
\caption{Description of the binary file outputted by the corpus preprocessor}
\label{fig:binary_format}
\end{figure}

The CAL system takes as input the corpus features (the binary output file of the
corpus preprocessor) and loads it in the memory. The document feature vectors
are indexed and stored in the memory throughout the lifetime of a CAL process in order to
ensure fast operations and reduce disk access. A review task is initiated by one
or more seed queries (such as the topic description) provided by the user. The
user can also specify seed document judgments and the choice of refresh strategy
at the beginning of a task. Users can restart the review task from any
checkpoint by just specifying the relevance judgments made until that time as
the seed document judgments. The system handles all the review tasks concurrently, so that
multiple users can use the system at the same time.

Training and scoring all the documents constitutes the majority of the computation.
For training, we modified sofia-ml so that we can invoke its functions natively
through our code and make it compatible with our data structures. We also
stripped away unneeded parts from the sofia-ml source code. The weight vector
obtained from training is used to fetch top documents from a set (depending
on the refresh strategy). The score for a single document is obtained by
computing the dot product of the document feature vector and the trained
weight vector. The scoring of all the documents is parallelized across multiple
threads (8 by default). By default, the code compiles with the \texttt{-O3}
compiler optimisation flag, which contributes to significant reductions in the
running time of various computations.

The CAL system is designed such that it is easy to extend or modify parts of the
algorithm. Most of the refresh strategies mentioned in this thesis were
implemented by extending a class and overriding few methods.

There are multiple ways to interact with the CAL system. The robust and user-friendly
HTTP API should be used for most purposes. Python bindings for the CAL system is
also available (it is an abstraction over the HTTP API). The command line tool
can be used for testing and simulation purposes. Interacting with CAL is
documented in detail in the project's
repository\footnote{\url{https://github.com/HTAustin/HiCAL/blob/master/CALEngine/README.md}}.
We also provide docker configuration which automatically installs all
dependencies, configures the web server and spawn the required processes.
